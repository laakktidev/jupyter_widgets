{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b277367-9d04-4c35-abd6-cea42160a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18e00e7a1b3410ba0787aa77c7de0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=datetime.date(2025, 12, 9), description='Päivämäärä', step=1), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Interaktiiviset widgetit\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Päivämäärä',\n",
    "    value=datetime.today().date()\n",
    ")\n",
    "\n",
    "place_dropdown = widgets.Dropdown(\n",
    "    options=['Helsinki', 'Oulu', 'Piippola', 'Rovaniemi', 'Tampere'],\n",
    "    value='Piippola',\n",
    "    description='Paikka:'\n",
    ")\n",
    "\n",
    "param_select = widgets.SelectMultiple(\n",
    "    options=['t2m', 'ws_10min', 'rh', 'pressure', 'precipitation1h'],\n",
    "    value=('t2m', 'rh'),\n",
    "    description='Parametrit:'\n",
    ")\n",
    "\n",
    "fetch_button = widgets.Button(description='Hae säätiedot')\n",
    "output = widgets.Output()\n",
    "\n",
    "def fetch_weather(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # API-kyselyn rakentaminen widgettien arvoista\n",
    "        start = date_picker.value.strftime('%Y-%m-%dT00:00:00Z')\n",
    "        end = (date_picker.value + timedelta(days=1)).strftime('%Y-%m-%dT00:00:00Z')\n",
    "        \n",
    "        url = f\"https://opendata.fmi.fi/wfs?service=WFS&version=2.0.0&request=getFeature&storedquery_id=fmi::observations::weather::timevaluepair&place={place_dropdown.value}&starttime={start}&endtime={end}&parameters={','.join(param_select.value)}\"\n",
    "        \n",
    "        # Data hakeminen ja visualisointi...\n",
    "        print(f\"Haetaan dataa: {url}\")\n",
    "        # ... lisää koodia datan käsittelyyn\n",
    "\n",
    "fetch_button.on_click(fetch_weather)\n",
    "\n",
    "# Asettelu\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([date_picker, place_dropdown]),\n",
    "    param_select,\n",
    "    fetch_button,\n",
    "    output\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b6f8e0d-da1e-4c25-9edc-15a9c63ae2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated token: eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ3dE9hV1o2aFJJeUowbGlsYXctcWd4NzlUdm1hX3ZKZlNuMW1WNm5HX0tVIn0.eyJleHAiOjE3NjUxNjk4ODIsImlhdCI6MTc2NTE2NjI4MiwianRpIjoiMzc1MjI3YTUtYmExMi00ZmE2LWEyZmMtNjc1ODNjZDU1NTJhIiwiaXNzIjoiaHR0cHM6Ly9zZXJ2aWNlcy5zZW50aW5lbC1odWIuY29tL2F1dGgvcmVhbG1zL21haW4iLCJhdWQiOiJodHRwczovL2FwaS5wbGFuZXQuY29tLyIsInN1YiI6ImVjMmJjZjg1LTM3MGYtNDRiMS05M2UzLTVlYTdlY2FkYTIwYyIsInR5cCI6IkJlYXJlciIsImF6cCI6Ijk0ZjczN2RhLThjNDUtNGIyOC1hNGY0LTliNTNmYjAwYjZjYyIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImNsaWVudEhvc3QiOiI0Ni4xMzIuODIuMjQ1IiwiZW1haWxfdmVyaWZpZWQiOmZhbHNlLCJwbF9wcm9qZWN0IjoiZjE3MGRiZDMtMTNlZi00M2UyLTk5ZjAtMmFlNzllODliZTNiIiwicHJlZmVycmVkX3VzZXJuYW1lIjoic2VydmljZS1hY2NvdW50LTk0ZjczN2RhLThjNDUtNGIyOC1hNGY0LTliNTNmYjAwYjZjYyIsImNsaWVudEFkZHJlc3MiOiI0Ni4xMzIuODIuMjQ1IiwiY2xpZW50X2lkIjoiOTRmNzM3ZGEtOGM0NS00YjI4LWE0ZjQtOWI1M2ZiMDBiNmNjIiwiYWNjb3VudCI6ImYxNzBkYmQzLTEzZWYtNDNlMi05OWYwLTJhZTc5ZTg5YmUzYiIsInBsX3dvcmtzcGFjZSI6ImY2MDQxNGFiLTk5ZWQtNDA5MS1iM2U4LTliMDE5NzFjYThmNyJ9.LZlJesb4xBX-V3Nf-Pn1NTdu3czB5f4w86Ns6M7rk1Vk1Iv-Iy0Ms7AXGdBzsmTWwHJ9YYoV30Eb0G58LeGlhUtfXWEsP_1edqtHw9Nl7FymyGUsRkZlxJo2qbdzHLyfqXshAS_Lo3IRuKSV0NuqzeUPLVSaxv1NeU4suCon2bP6XZzer4TsyeiM6qW7oZ6JHQf5JgdIQZwm8gdNZBhce-1s30KcKOOsgYj00_uURYg_WUtqwdspHy5OxK_7CZ6yL5pUU_qxjHRKBB0D7LHA-KqMvjhuWtbSSbWMe1-YaUFYUe38Y4nTQ-IA7S0DRRgqorwOCziUJSHPQXzknXV6dA\n",
      "Loaded NDVI from CSV cache: data\\ndvi_2017-05-01_2025-09-30.csv\n",
      "   year  day_of_year       min       max      mean  std  mean_smooth\n",
      "0  2017          121 -0.043881 -0.043881 -0.043881  0.0    -0.009074\n",
      "1  2017          125  0.084971  0.084971  0.084971  0.0     0.029222\n",
      "2  2017          128  0.035861  0.035861  0.035861  0.0     0.052491\n",
      "3  2017          131  0.106517  0.106517  0.106517  0.0     0.060735\n",
      "4  2017          138 -0.019001 -0.019001 -0.019001  0.0     0.032582\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentinelhub import (\n",
    "    SentinelHubStatistical, Geometry, DataCollection, CRS, SHConfig\n",
    ")\n",
    "from scipy.signal import savgol_filter\n",
    "import requests\n",
    "\n",
    "# ---------------- CONFIG -------------------\n",
    "os.environ[\"SH_CLIENT_ID\"] = \"94f737da-8c45-4b28-a4f4-9b53fb00b6cc\"\n",
    "os.environ[\"SH_CLIENT_SECRET\"] = \"yRGfObu8gGoDTKOnxVj1S5t7xwBTyOzG\"\n",
    "\n",
    "# ---------------- NDVI CLIENT -------------------\n",
    "class NDVIStatisticsClient:\n",
    "    def __init__(self, client_id, client_secret, resolution=10, data_dir=\"data\"):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.resolution = resolution\n",
    "\n",
    "        # Ensure data folder exists\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "        # Load SH credentials\n",
    "        self.config = SHConfig()\n",
    "        self.config.sh_client_id = client_id\n",
    "        self.config.sh_client_secret = client_secret\n",
    "        self.config.instance_id = os.getenv(\"SH_INSTANCE_ID\")\n",
    "\n",
    "        self.token = self._authenticate()\n",
    "        print(\"Authenticated token:\", self.token)\n",
    "\n",
    "    def _authenticate(self):\n",
    "        url = \"https://services.sentinel-hub.com/oauth/token\"\n",
    "        data = {\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": self.client_id,\n",
    "            \"client_secret\": self.client_secret\n",
    "        }\n",
    "        resp = requests.post(url, data=data)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()[\"access_token\"]\n",
    "\n",
    "    def _build_stat_request(self, geometry_geojson, date_from, date_to):\n",
    "        ndvi_evalscript = \"\"\"\n",
    "        //VERSION=3\n",
    "        function setup() {\n",
    "            return {\n",
    "                input: [\"B04\",\"B08\",\"dataMask\"],\n",
    "                output: [\n",
    "                    {id:\"NDVI\", bands:1, sampleType:\"FLOAT32\"},\n",
    "                    {id:\"dataMask\", bands:1}\n",
    "                ]\n",
    "            };\n",
    "        }\n",
    "        function evaluatePixel(sample) {\n",
    "            let ndvi = (sample.B08 - sample.B04)/(sample.B08 + sample.B04);\n",
    "            return {NDVI:[ndvi], dataMask:[sample.dataMask]};\n",
    "        }\n",
    "        \"\"\"\n",
    "        geom_obj = Geometry(geometry_geojson, crs=CRS.WGS84)\n",
    "\n",
    "        aggregation = SentinelHubStatistical.aggregation(\n",
    "            evalscript=ndvi_evalscript,\n",
    "            time_interval=(date_from, date_to),\n",
    "            aggregation_interval=\"P1D\",\n",
    "            resolution=(self.resolution, self.resolution)\n",
    "        )\n",
    "\n",
    "        input_data = SentinelHubStatistical.input_data(DataCollection.SENTINEL2_L2A)\n",
    "\n",
    "        request = SentinelHubStatistical(\n",
    "            aggregation=aggregation,\n",
    "            input_data=[input_data],\n",
    "            geometry=geom_obj,\n",
    "            config=self.config\n",
    "        )\n",
    "        return request\n",
    "\n",
    "    def _get_save_paths(self, date_from, date_to):\n",
    "        base = f\"ndvi_{date_from}_{date_to}\"\n",
    "        csv_file = os.path.join(self.data_dir, base + \".csv\")\n",
    "        json_file = os.path.join(self.data_dir, base + \".json\")\n",
    "        return csv_file, json_file\n",
    "\n",
    "    def _load_from_disk(self, csv_file, json_file):\n",
    "        if os.path.exists(csv_file):\n",
    "            print(f\"Loaded NDVI from CSV cache: {csv_file}\")\n",
    "            df = pd.read_csv(csv_file)\n",
    "            return df\n",
    "        if os.path.exists(json_file):\n",
    "            print(f\"Loaded NDVI from JSON cache: {json_file}\")\n",
    "            df = pd.read_json(json_file)\n",
    "            return df\n",
    "        return None\n",
    "\n",
    "    def _save_to_disk(self, df, csv_file, json_file):\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        df.to_json(json_file, orient=\"records\", indent=2)\n",
    "        print(f\"Saved NDVI to: {csv_file}\")\n",
    "        print(f\"Saved NDVI to: {json_file}\")\n",
    "\n",
    "    # ---------------- MAIN METHOD -------------------\n",
    "    def fetch_statistics(\n",
    "        self, geometry_geojson, date_from, date_to, smooth_method='savgol',\n",
    "        window=7, polyorder=2, fill_missing=True\n",
    "    ):\n",
    "        # Build save paths\n",
    "        csv_file, json_file = self._get_save_paths(date_from, date_to)\n",
    "\n",
    "        # Load cache if exists\n",
    "        df_cached = self._load_from_disk(csv_file, json_file)\n",
    "        if df_cached is not None:\n",
    "            return df_cached\n",
    "\n",
    "        # Fetch new data\n",
    "        req = self._build_stat_request(geometry_geojson, date_from, date_to)\n",
    "        try:\n",
    "            data = req.get_data()\n",
    "        except Exception as e:\n",
    "            print(\"Request FAILED:\", e)\n",
    "            return pd.DataFrame()\n",
    "        if not data:\n",
    "            print(\"No data returned\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse into dataframe\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                'date': pd.to_datetime(entry['interval']['from']),\n",
    "                'min': stats['min'],\n",
    "                'max': stats['max'],\n",
    "                'mean': stats['mean'],\n",
    "                'std': stats.get('stDev', stats.get('std', 0))\n",
    "            }\n",
    "            for entry in data[0]['data']\n",
    "            for stats in [list(entry['outputs']['NDVI']['bands'].values())[0]['stats']]\n",
    "        ])\n",
    "\n",
    "        # Fill missing NDVI values\n",
    "        if fill_missing:\n",
    "            df['mean'] = df['mean'].astype(float).interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # Smooth NDVI\n",
    "        if smooth_method == 'savgol' and len(df) >= window:\n",
    "            df['mean_smooth'] = savgol_filter(df['mean'], window_length=window, polyorder=polyorder)\n",
    "        else:\n",
    "            df['mean_smooth'] = df['mean']\n",
    "\n",
    "        # Growing season: May–September\n",
    "        df = df[(df['date'].dt.month >= 5) & (df['date'].dt.month <= 9)].copy()\n",
    "        df.sort_values('date', inplace=True)\n",
    "\n",
    "        # Add year and day_of_year for ML\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "        # Drop full date column to simplify ML usage\n",
    "        df = df[['year', 'day_of_year', 'min', 'max', 'mean', 'std', 'mean_smooth']]\n",
    "\n",
    "        # Save results\n",
    "        self._save_to_disk(df, csv_file, json_file)\n",
    "\n",
    "        return df\n",
    "\n",
    "# ---------------- USAGE EXAMPLE -------------------\n",
    "geometry = {\"type\":\"MultiPolygon\",\"coordinates\":[[[[26.878322063887,63.311237791188],[26.878210383755,63.311127053264],[26.878350385446,63.311033106844],[26.879191192544,63.310979667176],[26.879464866906,63.310962379787],[26.879701680754,63.310941138748],[26.879847289018,63.310917837033],[26.880211445045,63.31078830355],[26.881206228635,63.31039664547],[26.881288579858,63.310353284512],[26.881532572942,63.310360712245],[26.881700829469,63.310370410216],[26.881797056049,63.310339750986],[26.882334568098,63.310095692354],[26.882797119888,63.309911387811],[26.883108448744,63.309807121688],[26.883463636969,63.309756533726],[26.883510383732,63.309757936112],[26.883673739027,63.309918450969],[26.883738674074,63.310041199805],[26.883784434878,63.310094278776],[26.883952850997,63.310257534773],[26.884078174829,63.310359313368],[26.884149337208,63.31041777986],[26.884321316128,63.310559408797],[26.884318846541,63.310598016814],[26.883935071027,63.310748599924],[26.883509168981,63.31095297904],[26.883317387935,63.311025572764],[26.881677637236,63.311344490757],[26.881007972799,63.311457744175],[26.880906097594,63.311386120194],[26.880730006524,63.311324898811],[26.880563538684,63.311354242012],[26.880334914207,63.311392193646],[26.880353635611,63.311545303198],[26.880425469348,63.311556241036],[26.879749228825,63.311612277019],[26.87902085187,63.311685030682],[26.87870141608,63.311691993064]]]]}\n",
    "\n",
    "CLIENT_ID = os.getenv('SH_CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('SH_CLIENT_SECRET')\n",
    "\n",
    "client = NDVIStatisticsClient(CLIENT_ID, CLIENT_SECRET)\n",
    "\n",
    "df = client.fetch_statistics(\n",
    "    geometry,\n",
    "    \"2017-05-01\",\n",
    "    \"2025-09-30\"\n",
    ")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de725eca-3292-46b4-95c4-ba0d12640724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose df_all contains all years 2017-2025\n",
    "# Columns: ['year', 'day_of_year', 'min', 'max', 'mean', 'std', 'mean_smooth']\n",
    "\n",
    "# Use only relevant columns\n",
    "df_ml = df[['year', 'day_of_year', 'mean_smooth']].copy()\n",
    "\n",
    "# Features and target\n",
    "X = df_ml[['year', 'day_of_year']]\n",
    "y = df_ml['mean_smooth']\n",
    "\n",
    "# Split into training (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1385b9-0886-457e-a833-c6d3ae5e83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0165, R2: 0.6489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}, R2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7ea4eb6-422c-4e46-9453-65eacb1d9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  day_of_year  predicted_ndvi\n",
      "0  2026          121        0.148504\n",
      "1  2026          122        0.127646\n",
      "2  2026          123        0.113221\n",
      "3  2026          124        0.082679\n",
      "4  2026          125        0.052179\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# All days of growing season (May–Sept)\n",
    "days = np.arange(121, 274)  # May 1 = day 121, Sep 30 = day 273\n",
    "years = np.array([2026]*len(days))\n",
    "\n",
    "X_future = pd.DataFrame({'year': years, 'day_of_year': days})\n",
    "\n",
    "ndvi_pred = rf.predict(X_future)\n",
    "\n",
    "df_future = pd.DataFrame({\n",
    "    'year': years,\n",
    "    'day_of_year': days,\n",
    "    'predicted_ndvi': ndvi_pred\n",
    "})\n",
    "\n",
    "print(df_future.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06d54-1058-4241-a8ba-0afbfc14d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ----------------- Load NDVI JSON -----------------\n",
    "json_file = \"data/ndvi_2017-05-01_2025-09-30.json\"\n",
    "df = pd.read_json(json_file)\n",
    "\n",
    "# Ensure proper types\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['day_of_year'] = df['day_of_year'].astype(int)\n",
    "df['mean_smooth'] = df['mean_smooth'].astype(float)\n",
    "\n",
    "# Sort by year and day_of_year\n",
    "df = df.sort_values(['year', 'day_of_year']).reset_index(drop=True)\n",
    "\n",
    "# ----------------- Feature Engineering -----------------\n",
    "# Lag feature: previous day's NDVI\n",
    "df['NDVI_lag1'] = df['mean_smooth'].shift(1)\n",
    "\n",
    "# Remove NaN (first row of each year may have missing lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Features and target\n",
    "X = df[['day_of_year', 'NDVI_lag1']]\n",
    "y = df['mean_smooth']\n",
    "\n",
    "# ----------------- Train/Test Split -----------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# ----------------- Random Forest with hyperparameter tuning -----------------\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_dist,\n",
    "    n_iter=20, cv=3, scoring='r2', n_jobs=-1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# ----------------- Evaluate -----------------\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# ----------------- Predict for 2026 -----------------\n",
    "days_2026 = np.arange(121, 274)  # DOY for May–Sept\n",
    "# Use last known NDVI of 2025 as lag for first day\n",
    "last_ndvi_2025 = df[df['year'] == 2025]['mean_smooth'].iloc[-1]\n",
    "\n",
    "# Initialize lag list\n",
    "lag_ndvi = [last_ndvi_2025]\n",
    "\n",
    "# Iteratively predict day by day using lag feature\n",
    "y_pred_2026 = []\n",
    "for day in days_2026:\n",
    "    X_pred = pd.DataFrame({'day_of_year': [day], 'NDVI_lag1': [lag_ndvi[-1]]})\n",
    "    y_day = best_rf.predict(X_pred)[0]\n",
    "    y_pred_2026.append(y_day)\n",
    "    lag_ndvi.append(y_day)\n",
    "\n",
    "df_pred_2026 = pd.DataFrame({'year': 2026, 'day_of_year': days_2026, 'NDVI_pred': y_pred_2026})\n",
    "print(df_pred_2026.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
